{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for Reading and Datamining PDB Structures with mmtf-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from mmtfPyspark.filters import ContainsGroup\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.interactions import InteractionExtractor, InteractionFilter\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the benchmark\n",
    "Set the path to the MMTF Hadoop Sequence file. Here we retrieve the value of the environment variable MMTF_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Sequence file path: MMTF_FULL=/Users/peter/MMTF_Files/full\n"
     ]
    }
   ],
   "source": [
    "path = mmtfReader.get_mmtf_full_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a list with the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cores = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results directory\n",
    "results_dir = '../results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Benchmark\n",
    "Benchmarks reading an MMTF Hadoop Sequence File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    count = structures.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read, cores: 4 time: 229.9848439693451 seconds\n"
     ]
    }
   ],
   "source": [
    "df_read = pd.DataFrame(columns=('cores', 'read'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = read(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('read, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_read = df_read.append([{'cores':num_cores, 'read': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.to_csv(os.path.join(results_dir, 'read.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>229.984844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count        read\n",
       "0     4  140825.0  229.984844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions Benchmark\n",
    "This benchmark finds all zinc interactions in PDB structures. Structures with multiple models, e.g., NMR structures are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Interactions\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    structures = structures.filter(lambda s: s[1].num_models == 1)\n",
    "    structures = structures.filter(ContainsGroup('ZN'))\n",
    "                               \n",
    "    interaction_filter = InteractionFilter()\n",
    "    interaction_filter.set_target_elements(False, ['C','H','P'])\n",
    "    interaction_filter.set_query_elements(True, ['Zn'])\n",
    "    interaction_filter.set_distance_cutoff(3.0)\n",
    "\n",
    "    interactions = InteractionExtractor().get_ligand_polymer_interactions(structures, interaction_filter)\n",
    "    count = interactions.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions, cores: 4 time: 311.6189091205597 seconds\n"
     ]
    }
   ],
   "source": [
    "df_interactions = pd.DataFrame(columns=('cores', 'interactions'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = interactions(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('interactions, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_interactions = df_interactions.append([{'cores':num_cores, 'interactions': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.to_csv(os.path.join(results_dir, 'interactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>127196.0</td>\n",
       "      <td>311.618909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  interactions\n",
       "0     4  127196.0    311.618909"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saltbridges Benchmark\n",
    "This benchmark finds salt bridges in protein structures. Structures with multiple models, e.g., NMR structures are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saltbridges(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Saltbridges\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    structures = structures.filter(lambda s: s[1].num_models == 1)\n",
    "                               \n",
    "    salt_bridge = InteractionFilter(distanceCutoff=3.5)\n",
    "    salt_bridge.set_query_groups(True, ['ASP', 'GLU'])\n",
    "    salt_bridge.set_query_atom_names(True, ['OD1', 'OD2', 'OE1', 'OE2'])\n",
    "    salt_bridge.set_target_groups(True, ['ARG', 'LYS', 'HIS'])\n",
    "    salt_bridge.set_target_atom_names(True, ['NH1', 'NH2', 'NZ', 'ND1', 'NE2'])\n",
    "\n",
    "    interactions = InteractionExtractor.get_polymer_interactions(structures, salt_bridge)\n",
    "    count = interactions.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saltbridges, cores: 4 time: 1050.5137100219727 seconds\n"
     ]
    }
   ],
   "source": [
    "df_saltbridges = pd.DataFrame(columns=('cores', 'saltbridges'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = saltbridges(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('saltbridges, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_saltbridges = df_saltbridges.append([{'cores':num_cores, 'saltbridges': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>saltbridges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>523868.0</td>\n",
       "      <td>1050.51371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  saltbridges\n",
       "0     4  523868.0   1050.51371"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saltbridges.to_csv(os.path.join(results_dir, 'saltbridges.csv'), index=False)\n",
    "df_saltbridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MMTF Hadoop Sequence File\n",
    "This benchmarks read the raw MMTF Hadoop Sequence file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)\n",
    "    count = rdd.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_hadoop, cores: 4 time: 70.44136500358582 seconds\n"
     ]
    }
   ],
   "source": [
    "df_read_hadoop = pd.DataFrame(columns=('cores', 'read_hadoop'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = read_hadoop(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('read_hadoop, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_read_hadoop = df_read_hadoop.append([{'cores':num_cores, 'read_hadoop': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>read_hadoop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>70.441365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  read_hadoop\n",
       "0     4  140825.0    70.441365"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read_hadoop.to_csv(os.path.join(results_dir, 'read_hadoop.csv'), index=False)\n",
    "df_read_hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and and Unzip Data in MMTF Hadoop Sequence File\n",
    "This benchmark reads the MMTF Hadoop Sequence File and unzips the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    count = data.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip, cores: 4 time: 110.27356004714966 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unzip = pd.DataFrame(columns=('cores', 'unzip'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unzip(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unzip, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unzip = df_unzip.append([{'cores':num_cores, 'unzip': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unzip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>110.27356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count      unzip\n",
       "0     4  140825.0  110.27356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unzip.to_csv(os.path.join(results_dir, 'unzip.csv'), index=False)\n",
    "df_unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the Pandas libarary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_pd(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: pd.read_msgpack(d))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack_pd, cores: 4 time: 131.6141209602356 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unpack_pd = pd.DataFrame(columns=('cores', 'unpack_pd'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_pd(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_pd, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_pd = df_unpack_pd.append([{'cores':num_cores, 'unpack_pd': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unpack_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>131.614121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count   unpack_pd\n",
       "0     4  140825.0  131.614121"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpack_pd.to_csv(os.path.join(results_dir, 'unpack_pd.csv'), index=False)\n",
    "df_unpack_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data using MsgPack\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the msgpack library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "\n",
    "def unpack_msgpack(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: msgpack.unpackb(d, raw=False))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack_msgpack, cores: 4 time: 436.3425590991974 seconds\n"
     ]
    }
   ],
   "source": [
    "df_unpack_msgpack = pd.DataFrame(columns=('cores', 'unpack_msgpack'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_msgpack(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_msgpack, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_msgpack = df_unpack_msgpack.append([{'cores':num_cores, 'unpack_msgpack': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>unpack_msgpack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>436.342559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  unpack_msgpack\n",
       "0     4  140825.0      436.342559"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpack_msgpack.to_csv(os.path.join(results_dir, 'unpack_msgpack.csv'), index=False)\n",
    "df_unpack_msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mmtf-pyspark]",
   "language": "python",
   "name": "conda-env-mmtf-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
