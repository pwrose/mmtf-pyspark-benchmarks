{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for decoding mmtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from mmtfPyspark.filters import ContainsGroup\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.utils import DsspSecondaryStructure\n",
    "from mmtfPyspark.utils import MmtfStructure\n",
    "\n",
    "from mmtfPyspark.interactions import InteractionExtractor, InteractionFilter\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the benchmark\n",
    "Set the path to the MMTF Hadoop Sequence file. Here we retrieve the value of the environment variable MMTF_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Sequence file path: MMTF_FULL=/Users/peter/MMTF_Files/full\n"
     ]
    }
   ],
   "source": [
    "path = mmtfReader.get_mmtf_full_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a list with the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cores = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results directory\n",
    "results_dir = '../results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MmtfChain decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmtf_chain(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: pd.read_msgpack(d))\n",
    "    structure = unpack.map(lambda u: MmtfStructure(u))\n",
    "    #chains = structure.flatMap(lambda s: s.get_chains())\n",
    "    acount = structure.map(lambda c: len(c.group_numbers))\n",
    "    count = acount.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmtf_chain, cores: 4 time: 396.53967809677124 seconds\n"
     ]
    }
   ],
   "source": [
    "df_mmtf_chain = pd.DataFrame(columns=('cores', 'mmtf_chain'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = mmtf_chain(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('mmtf_chain, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_mmtf_chain = df_mmtf_chain.append([{'cores':num_cores, 'mmtf_chain': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cores</th>\n",
       "      <th>count</th>\n",
       "      <th>mmtf_chain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>140825.0</td>\n",
       "      <td>396.539678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cores     count  mmtf_chain\n",
       "0     4  140825.0  396.539678"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mmtf_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import msgpack\n",
    "import gzip\n",
    "from mmtfPyspark.utils import MmtfStructure\n",
    "from mmtfPyspark.utils import MmtfChain\n",
    "from mmtf.api import default_api\n",
    "from os import path, walk\n",
    "from pyspark.sql import SparkSession\n",
    "import urllib\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MmtfStructure' object has no attribute 'modelToGroupsIndices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-94cf4b4f05d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0munpack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_msgpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstructure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMmtfStructure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mmtf-pyspark-benchmarks/lib/python3.7/site-packages/mmtfPyspark/utils/mmtfStructure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelToGroupIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelToChainIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentityChainIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain_to_entity_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mmtf-pyspark-benchmarks/lib/python3.7/site-packages/mmtfPyspark/utils/mmtfStructure.py\u001b[0m in \u001b[0;36mcalc_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchainToGroupIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchainCount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelToAtomIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matomCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelToGroupsIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelToChainIndices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MmtfStructure' object has no attribute 'modelToGroupsIndices'"
     ]
    }
   ],
   "source": [
    "pdbId = \"1J6T\"\n",
    "#pdbId = \"1OHR\"\n",
    "url = default_api.get_url(pdbId, False)\n",
    "request = urllib2.Request(url)\n",
    "request.add_header('Accept-encoding', 'gzip')\n",
    "response = urllib2.urlopen(request)\n",
    "if response.info().get('Content-Encoding') == 'gzip':\n",
    "    data = gzip.decompress(response.read())\n",
    "else:\n",
    "    data = response.read()\n",
    "unpack = pd.read_msgpack(data)\n",
    "structure = MmtfStructure(unpack)\n",
    "chain = structure.get_chain('A')\n",
    "print(chain.start, chain.end)\n",
    "print(chain.chain_name)\n",
    "print('atoms:', chain.num_atoms)\n",
    "print('groups:', chain.num_groups)\n",
    "print('chains:', chain.num_chains)\n",
    "print('models:', chain.num_models)\n",
    "print('models:', chain.elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and and Unzip Data in MMTF Hadoop Sequence File\n",
    "This benchmark reads the MMTF Hadoop Sequence File and unzips the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    count = data.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unzip = pd.DataFrame(columns=('cores', 'unzip'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unzip(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unzip, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unzip = df_unzip.append([{'cores':num_cores, 'unzip': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unzip.to_csv(os.path.join(results_dir, 'unzip.csv'), index=False)\n",
    "df_unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the Pandas libarary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_pd(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: pd.read_msgpack(d))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpack_pd = pd.DataFrame(columns=('cores', 'unpack_pd'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_pd(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_pd, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_pd = df_unpack_pd.append([{'cores':num_cores, 'unpack_pd': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpack_pd.to_csv(os.path.join(results_dir, 'unpack_pd.csv'), index=False)\n",
    "df_unpack_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Data using MsgPack\n",
    "This benchmark read an MMTF Hadoop Sequence File, unzips the data, and decodes the data using the msgpack library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "\n",
    "def unpack_msgpack(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Read\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    text = \"org.apache.hadoop.io.Text\"\n",
    "    byteWritable = \"org.apache.hadoop.io.BytesWritable\"\n",
    "    rdd = sc.sequenceFile(path, text, byteWritable)  ## returns key/value tuples\n",
    "    data = rdd.map(lambda t: gzip.decompress(t[1]))  # t[1] are the values in the rdd\n",
    "    unpack = data.map(lambda d: msgpack.unpackb(d, raw=False))\n",
    "    count = unpack.count()\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpack_msgpack = pd.DataFrame(columns=('cores', 'unpack_msgpack'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = unpack_msgpack(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('unpack_msgpack, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_unpack_msgpack = df_unpack_msgpack.append([{'cores':num_cores, 'unpack_msgpack': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpack_msgpack.to_csv(os.path.join(results_dir, 'unpack_msgpack.csv'), index=False)\n",
    "df_unpack_msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
