{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark for Reading and Datamining PDB Structures with mmtf-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.filters import ContainsGroup\n",
    "from mmtfPyspark.utils import ColumnarStructure\n",
    "from mmtfPyspark.interactions import InteractionExtractorPd\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the benchmark\n",
    "Set the path to the MMTF Hadoop Sequence file. Here we retrieve the value of the environment variable MMTF_FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = mmtfReader.get_mmtf_full_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a list with the number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cores = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results directory\n",
    "results_dir = '../results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Benchmark\n",
    "This benchmark read structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Benchmark3\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path, first_model=True)                \n",
    "    count = structures.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s = pd.DataFrame(columns=('cores', 'structures'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = structure(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('structures, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_s = df_s.append([{'cores':num_cores, 'structures': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.to_csv(os.path.join(results_dir, 'structures.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"Benchmark3\").getOrCreate()\n",
    "#structures = mmtfReader.read_sequence_file(path, first_model=True)\n",
    "structures = mmtfReader.download_full_mmtf_files(['4HHB'])\n",
    "#structures = structures.filter(lambda s: s[0] == '4HHB')\n",
    "dfs = structures.map(lambda s: s[1]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.DataFrame({'chain_name': dfs.chain_names,\n",
    "#                                     'chain_id': dfs.chain_ids,\n",
    "#                                     'group_number': dfs.group_numbers,\n",
    "#                                     'group_name': dfs.group_names,\n",
    "#                                     'atom_name': dfs.atom_names,\n",
    "#                                     'altloc': dfs.alt_loc_list,\n",
    "#                                     'x': dfs.x_coord_list,\n",
    "#                                     'y': dfs.y_coord_list,\n",
    "#                                     'z': dfs.z_coord_list,\n",
    "#                                     'o': dfs.occupancy_list,\n",
    "#                                     'b': dfs.b_factor_list,\n",
    "#                                     'element': dfs.elements,\n",
    "#                                     'polymer:': dfs.polymer\n",
    "#                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'z': dfs.z_coord_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chain_name'] = df['chain_name'].astype('category')\n",
    "df['group_name'] = df['group_name'].astype('category')\n",
    "df['atom_name'] = df['atom_name'].astype('category')\n",
    "df['altloc'] = df['altloc'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure To Pandas Benchmark\n",
    "This benchmark read structures and converts them to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Benchmark3\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path, first_model=True)\n",
    "    dfs = structures.map(lambda s: s[1].to_pandas())\n",
    "    count = dfs.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s = pd.DataFrame(columns=('cores', 'structures_to_pandas'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = structure(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('structures, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_s = df_s.append([{'cores':num_cores, 'structures_to_pandas': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.to_csv(os.path.join(results_dir, 'structures_to_pandas.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure to Chain Benchmark\n",
    "This benchmark read structures and flatmaps to polymer chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_to_chains(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Interactions\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    chains = structures.flatMap(lambda s: s[1].get_chains())                 \n",
    "    count = chains.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s2c = pd.DataFrame(columns=('cores', 'structure_to_chains'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = structure_to_chains(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('structure_to_chains, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_s2c = df_s2c.append([{'cores':num_cores, 'structure_to_chains': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s2c.to_csv(os.path.join(results_dir, 'structure_to_chains.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure to Chain to pandas Benchmark\n",
    "This benchmark read structures and flatmaps to polymer chains and convert to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_to_chains(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Interactions\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    chains = structures.flatMap(lambda s: s[1].get_chains())\n",
    "    dfc = chains.map(lambda c: c.to_pandas())\n",
    "    count = dfc.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_s2c = pd.DataFrame(columns=('cores', 'structure_to_chains'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = structure_to_chains(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('structure_to_chains, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_s2c = df_s2c.append([{'cores':num_cores, 'structure_to_chains': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s2c.to_csv(os.path.join(results_dir, 'structure_to_chains_to_pandas.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saltbridges Benchmark\n",
    "This benchmark finds salt bridges in protein structures. Structures with multiple models, e.g., NMR structures are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saltbridges(path, num_cores):\n",
    "    spark = SparkSession.builder.master(\"local[\" + str(num_cores) + \"]\").appName(\"Saltbridges\").getOrCreate()\n",
    "    structures = mmtfReader.read_sequence_file(path)\n",
    "    structures = structures.filter(lambda s: s[1].num_models == 1)\n",
    "                               \n",
    "    distance_cutoff = 3.5\n",
    "    query = \"polymer and (group_name in ['ASP', 'GLU']) and (atom_name in ['OD1', 'OD2', 'OE1', 'OE2'])\"\n",
    "    target = \"polymer and (group_name in ['ARG', 'LYS', 'HIS']) and (atom_name in ['NH1', 'NH2', 'NZ', 'ND1', 'NE2'])\"\n",
    "\n",
    "    interactions = InteractionExtractorPd.get_interactions(structures, distance_cutoff, query, target, bio=None)\n",
    "    count = interactions.count()\n",
    "\n",
    "    spark.stop()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saltbridges = pd.DataFrame(columns=('cores', 'saltbridges_pd'))\n",
    "\n",
    "for num_cores in cores:\n",
    "    start = time.time()\n",
    "    count = saltbridges(path, num_cores)\n",
    "    end = time.time()\n",
    "    print('saltbridges_pd, cores:', num_cores, 'time:', end-start, 'seconds')\n",
    "    df_saltbridges = df_saltbridges.append([{'cores':num_cores, 'saltbridges_pd': end-start, 'count': count}], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saltbridges.to_csv(os.path.join(results_dir, 'saltbridges_pd.csv'), index=False)\n",
    "df_saltbridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
